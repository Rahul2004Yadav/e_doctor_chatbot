# -*- coding: utf-8 -*-
"""evaluate_Qlora_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rxQrgAS6qsIx3_gRVWlTQgTxL6-3Q8yR
"""

!pip install torch transformers peft bitsandbytes datasets evaluate

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/My Drive/e-doctor-qlora-adapter.zip" -d /content/

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

base_model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
adapter_path = "/content/e-doctor-qlora-adapter"

tokenizer = AutoTokenizer.from_pretrained(adapter_path)
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)
model = PeftModel.from_pretrained(base_model, adapter_path)
model.eval()

import json
json_path = '/content/test___qlora.json'
with open(json_path, 'r', encoding='utf-8') as f:
    test_data = json.load(f)

!pip install rouge_score

import evaluate

rouge = evaluate.load('rouge')
predictions = []
references = []

for example in test_data:
    prompt = example['input']
    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=False
        )
    pred = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    predictions.append(pred)
    references.append(example['output'])

rouge_results = rouge.compute(predictions=predictions, references=references)
print("ROUGE:", rouge_results)

import math

def calculate_ppl(model, tokenizer, text):
    encodings = tokenizer(text, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model(**encodings, labels=encodings.input_ids)
        loss = outputs.loss
    return math.exp(loss.item())

ppl_scores = []
for example in test_data:
    ppl = calculate_ppl(model, tokenizer, example["output"])
    ppl_scores.append(ppl)
avg_ppl = sum(ppl_scores) / len(ppl_scores)
print("Average Perplexity:", avg_ppl)

import time

latencies = []
for example in test_data:
    prompt = example['input']
    inputs = tokenizer(prompt, return_tensors='pt').to(model.device)
    start_time = time.time()
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=100,
            do_sample=False
        )
    end_time = time.time()
    latencies.append(end_time - start_time)
avg_latency = sum(latencies) / len(latencies)
print("Average Latency (seconds):", avg_latency)

import os

def get_dir_size(path):
    total = 0
    for dirpath, dirnames, filenames in os.walk(path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            total += os.path.getsize(fp)
    return total

adapter_size = get_dir_size(adapter_path) / (1024 ** 2)

print(f"Adapter size: {adapter_size:.2f} MB")

import pandas as pd

metrics = ['ROUGE-1', 'ROUGE-2', 'PPL', 'Latency (s)', 'Model Size (MB)']
values = [0.201933, 0.04892, 22.8, 15.726, 176.53]

description = [
    'Unigram overlap (measures recall of important words)',
    'Bigram overlap (measures recall of important word pairs)',
    'reflects model fluency and prediction quality',
    'Inference time per token',
    'Disk size of the fine-tuned model (smaller is more deployable)'
]

comparative_results_df = pd.DataFrame({
    'Metric': metrics,
    'Value': values,
    'Description': description
})


print(comparative_results_df.to_markdown(index=False))



