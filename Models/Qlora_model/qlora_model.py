# -*- coding: utf-8 -*-
"""QLORA_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j_L2sCou99jh5pIw_U3jtTaJvc-Q_Mmd
"""

# Install requirements
!pip install -q -U bitsandbytes git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/accelerate.git
!pip install -q -U datasets trl wandb

# Import libraries
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model
from trl import SFTTrainer
import torch
from datasets import Dataset
import wandb
import os
import json

#Initialize Weights & Biases
wandb.init(project="medical-llm-finetuning")

# Load model with QLoRA configuration
model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# Prepare model for QLoRA training
model = prepare_model_for_kbit_training(model)

# Configure QLoRA
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM",
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj"
    ]
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# Prepare dataset with correct formatting
def create_prompt(patient_query, doctor_response):
    return f"""You are a conversational AI assistant acting as a professionally worded "e-doctor." Your role is to provide preliminary, text-based guidance in response to patient symptom-related questions.

### Patient Query:
{patient_query}

### AI Doctor Response:
{doctor_response}

*Disclaimer: This response is generated for informational purposes only and should not be considered a substitute for professional medical advice, diagnosis, or treatment. Please consult a licensed healthcare provider for personalized care.*"""

# Load and format dataset
with open("/content/english-train.json", "r") as f:
    raw_data = json.load(f)

formatted_data = []
for entry in raw_data:
    if len(entry["utterances"]) >= 2:
        patient_utterance = entry["utterances"][0].replace("patient:", "").strip()
        doctor_response = entry["utterances"][1].replace("doctor:", "").strip()

        formatted_text = create_prompt(patient_utterance, doctor_response)
        formatted_data.append({"text": formatted_text})

dataset = Dataset.from_list(formatted_data)

training_args = TrainingArguments(
    output_dir="e-doctor-finetuned",
    max_steps=60,
    num_train_epochs=2,
    per_device_train_batch_size=2,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    fp16=True,
    optim="paged_adamw_8bit",
    logging_steps=10,
    save_strategy="epoch",
    report_to="wandb",

)


def formatting_func(example):
    return example["text"]

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    formatting_func=formatting_func,
    # max_seq_length=1024
)

# Train and save
trainer.train()
trainer.model.save_pretrained("e-doctor-qlora-adapter")
tokenizer.save_pretrained("e-doctor-qlora-adapter")

!zip -r e-doctor-qlora-adapter e-doctor-qlora-adapter

from google.colab import files
files.download("e-doctor-qlora-adapter.zip")

PROMPT_TEMPLATE = """You are a conversational AI assistant acting as a professionally worded "e-doctor." Your role is to provide preliminary, text-based guidance in response to patient symptom-related questions.

### Patient Query:
{question}

### AI Doctor Response:
"""

def get_e_doctor_response(patient_query, model, tokenizer,
                         max_new_tokens=100, temperature=0.7):
    # Format the prompt with the patient query
    prompt = PROMPT_TEMPLATE.format(question=patient_query)
    # Tokenize and move to the model's device
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    # Generate the response
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature
        )
    # Decode and return only the generated response (after the prompt)
    full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    response = full_output.split("### AI Doctor Response:")[-1].strip()
    return response

patient_query = "i have tuberculosis what should i do?"
response = get_e_doctor_response(patient_query, model, tokenizer)
print("AI Doctor:", response)

patient_query = "dry scratchy throat, very mild cough since 23 march but not improving. traveled to cape town via lanseria and ct airports 15-20 march. wife has sore throat, nasal congestion, lethargy & headaches since 21 march. should we see a doctor to be assessed?"

response = get_e_doctor_response(patient_query, model, tokenizer)
print("AI Doctor:", response)

