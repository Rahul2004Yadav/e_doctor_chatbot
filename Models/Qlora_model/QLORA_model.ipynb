{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },  
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1K45Xr08Q-A",
        "outputId": "bb1b0d5c-89d6-42a5-d091-67cffe59977a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m122.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m366.3/366.3 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install requirements\n",
        "!pip install -q -U bitsandbytes git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/peft.git git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets trl wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "import wandb\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "GTuHYZjf81rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize Weights & Biases\n",
        "wandb.init(project=\"medical-llm-finetuning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "n_6CR3ER83Lh",
        "outputId": "4c427e04-f9b4-4f8c-f6b0-b9c93ce5984b"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahul9664171057\u001b[0m (\u001b[33mrahul9664171057-iit-roorkee\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.20.1"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250605_171603-y1l1ptjk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rahul9664171057-iit-roorkee/medical-llm-finetuning/runs/y1l1ptjk' target=\"_blank\">dark-hill-1</a></strong> to <a href='https://wandb.ai/rahul9664171057-iit-roorkee/medical-llm-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rahul9664171057-iit-roorkee/medical-llm-finetuning' target=\"_blank\">https://wandb.ai/rahul9664171057-iit-roorkee/medical-llm-finetuning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rahul9664171057-iit-roorkee/medical-llm-finetuning/runs/y1l1ptjk' target=\"_blank\">https://wandb.ai/rahul9664171057-iit-roorkee/medical-llm-finetuning/runs/y1l1ptjk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/rahul9664171057-iit-roorkee/medical-llm-finetuning/runs/y1l1ptjk?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f40489add90>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model with QLoRA configuration\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414,
          "referenced_widgets": [
            "19404babdba9452d828690a389f9b702",
            "d3dd57b17f3a4bf5846a497cd6d389ed",
            "92c4905a054c42a6b36c18b97a0972f8",
            "a6e62c8d3ce74b7caa556d4bb94e325c",
            "640d2e1900d54a2eb02077cf0b407f15",
            "028662bebf784ad6824f57f350cdd388",
            "0f134213d5ca4efa8df986db4b9b45d9",
            "387542e25a9c496d94f4e5188714ce38",
            "7ff24e8267544ef5a6eed7ce045aa2ef",
            "1075635e705c4d3f9de79d2ec9884337",
            "8263c913e2974e2e8acc3112b715fc91",
            "7c1d6d0526cd4248a1990d98c89a9f02",
            "8c4bf4c7cf184f2b8cbbc57263562494",
            "8701932a34ca461e86e5d2feb5509488",
            "5529e3f09e9b458b8e3038b8da9ad06e",
            "e41ff66c38ad4d7f81d5e61ec80ca9e8",
            "0f7a07c271104eac9c2f1136e02c512a",
            "d9f730cc566c4b89b72d3eaeebb7eacc",
            "bda3be9335554b4d8b8c9a6a08d5df84",
            "1e760aa056d14034a3b4876de3f47d10",
            "71e620921ed541f291aa48914b625227",
            "b2d4793a37284b8792d26359420908bc",
            "0e4ee73df83f462ebd2deccd4670303e",
            "b94f0c636fc64521accd3a8fc0f5e012",
            "f2f458b04b4c48018a39c5875052b660",
            "b129544dc09042069b5cc002a6805c91",
            "0a944bfac0604c82a1ceb7cab0a8f7c6",
            "589869234fd94107b42c11bc77cdeeb3",
            "deeed29c854a4bdca7e209fedfdc9f15",
            "745b087472a34fab80be53ac9eb5a59f",
            "bb5c18c58b53495e9be3d05feb353593",
            "2581066551fc49d28c371bd0d984db42",
            "03bb05bf70eb4fa39b40009391cfefa7",
            "dcb15999ec214e348f99c0f37902c3aa",
            "1331e8bb6f8f486baaf83dc50bdddbbf",
            "2a09763036ba4344af4f24acf8aef902",
            "36dc23d288274af0874879d4f3f4fb4a",
            "965f649bdfe14a2aaccdcd5201d98147",
            "2d3b0b977f3f410da1efa34cae3cc4d6",
            "f5101fb4edeb43ac82f64cf99fb5b4e2",
            "4de67fdf3ec441968a474effd4a99d71",
            "d68774962ba44263b6d867a4b510a341",
            "30aa8ef7b8ae4ddcae4f7ef2cc15566d",
            "c374fd0141bb412aab4949f453836436",
            "1aa6c971c2734a428bd225d7e521fbab",
            "2ca3c4faba464ed3882946bc4f100046",
            "c4cc10971fc64295bf1998f895439d5c",
            "b1e7c5c4044a4e69b0a9b6a4c60643bd",
            "442f25730c3d4c1ebc164b75cea97822",
            "000cd6c57aaa44c78fe00e36b377b2c5",
            "f443da1ea2e1498d89bdc58e81e24e46",
            "dcbb74bbcfc146f7999ee6259923d79d",
            "1428a775bd424fe385e587019ecd5837",
            "c8f195299bc543afb673683658581421",
            "b811caedf0744256972102c9110f93de",
            "3c1c43a6f8be443aacc7726d4f3bf25d",
            "ad94044248ed47bfa3d503763e19a1b2",
            "57f504488d81478a80e24b130201dc2d",
            "cc7cb7621f3246a6b6e81cd3f4785c60",
            "9dba6d826bfb44d1936422d2b1181f71",
            "1894f125b95e43b681f9e89a1f25d274",
            "68c75c0620df4b2c8a217bca49082b7c",
            "a91189ce710a492f8b5c10dfa110f91f",
            "6eee72bbf02545febe379204b9e6cbf9",
            "43c054f7775f4fd2bd509c1651846598",
            "35497c37f8c54794899b44f19340a56d",
            "c475dcac81e045b7953441089539d52b",
            "e952dd1782db4f23a25aa4fc2d0c891d",
            "d253e68fa5de4d8d99f490390549ffb5",
            "242e5240c4f449e0bf7937e41ff70f08",
            "e2e42a1ccfe647bc965a650105fa0440",
            "bb301bbbd0d94318be0b471f39d19567",
            "8fc638f494be4367adacf1377cca0f13",
            "f4b5d101f580438c9353a661f3b23984",
            "b56234cd14934843be14fd3717f84f2d",
            "fcfe3f34232541ae9d9df54f4b9daab6",
            "7eeed6a1d7e4405f97d7d601fcf13070",
            "21c0a22cbaae411c892b3a5203b04dfd",
            "4496960a5f464b5f93653bb344da338d",
            "4f4088946afc4470879955116fd6ae5b",
            "3da1467a2c2941d38dac12ffd2a780f2",
            "0e0c9f12347046b0aa7a5f3351584616",
            "95947c8f048a4522b9eb5559cd41714d",
            "7a04c621cf3f49ba856e5ff369b1cc68",
            "c8a4f498d96a4d2b9a8364283b3b4cf3",
            "d066f2ab5d534e13a6f9c11ecc4551ad",
            "fe25c60583f4407b814a38e6a89b58d9",
            "62fa0e6c630f428e875540dffdb75002",
            "34c639bfec774854864b87c886a20ea4",
            "a2c0bac307ab48d8b532ada2d6c6d3f4",
            "2105771a47a44f49aef5a0c6b7b3b1bb",
            "15118b7582474536915119d2e6d2796f",
            "8aee3de0b5a042dcab6873c83e564893",
            "70fd02d8196b4a4aa8629d93a24fb74d",
            "2f56f5ecfc1f4256ae2bbcdb04027af2",
            "305aa6587c3547e5ba7e94db801e79e9",
            "334a3bddbd3043d095fc5e5eb615ca6a",
            "5713b0bdaf4b426ab6283a9c6ca06772",
            "77da99f3523145f4b452511c4e3a231c"
          ]
        },
        "id": "FzwUbg8k85ce",
        "outputId": "9b7e564b-514c-4415-b608-cbeb9a3a6b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19404babdba9452d828690a389f9b702"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c1d6d0526cd4248a1990d98c89a9f02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e4ee73df83f462ebd2deccd4670303e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcb15999ec214e348f99c0f37902c3aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1aa6c971c2734a428bd225d7e521fbab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-000002.safetensors:   0%|          | 0.00/8.67G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c1c43a6f8be443aacc7726d4f3bf25d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-000002.safetensors:   0%|          | 0.00/7.39G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c475dcac81e045b7953441089539d52b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21c0a22cbaae411c892b3a5203b04dfd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34c639bfec774854864b87c886a20ea4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare model for QLoRA training\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "sVOhpkjY86rG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure QLoRA\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vbe8AiE088CZ",
        "outputId": "4e878d57-aeb9-41a3-f13f-01d345980a07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 41,943,040 || all params: 8,072,204,288 || trainable%: 0.5196\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset with correct formatting\n",
        "def create_prompt(patient_query, doctor_response):\n",
        "    return f\"\"\"You are a conversational AI assistant acting as a professionally worded \"e-doctor.\" Your role is to provide preliminary, text-based guidance in response to patient symptom-related questions.\n",
        "\n",
        "### Patient Query:\n",
        "{patient_query}\n",
        "\n",
        "### AI Doctor Response:\n",
        "{doctor_response}\n",
        "\n",
        "*Disclaimer: This response is generated for informational purposes only and should not be considered a substitute for professional medical advice, diagnosis, or treatment. Please consult a licensed healthcare provider for personalized care.*\"\"\"\n",
        "\n",
        "# Load and format dataset\n",
        "with open(\"/content/english-train.json\", \"r\") as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "formatted_data = []\n",
        "for entry in raw_data:\n",
        "    if len(entry[\"utterances\"]) >= 2:\n",
        "        patient_utterance = entry[\"utterances\"][0].replace(\"patient:\", \"\").strip()\n",
        "        doctor_response = entry[\"utterances\"][1].replace(\"doctor:\", \"\").strip()\n",
        "\n",
        "        formatted_text = create_prompt(patient_utterance, doctor_response)\n",
        "        formatted_data.append({\"text\": formatted_text})\n",
        "\n",
        "dataset = Dataset.from_list(formatted_data)"
      ],
      "metadata": {
        "id": "MfIJx7xA89FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"e-doctor-finetuned\",\n",
        "    max_steps=60,\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"wandb\",\n",
        "\n",
        ")\n",
        "\n",
        "\n",
        "def formatting_func(example):\n",
        "    return example[\"text\"]\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    formatting_func=formatting_func,\n",
        "    # max_seq_length=1024\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215,
          "referenced_widgets": [
            "aad446ee48f04ff7891a5f71d3f54710",
            "7b8ebb5b70a14dfebe6b9268973e5d78",
            "484ec68c5d5e40c693be2d87483ee52b",
            "1322cdc3f5184f3abc2d46f21fe87b8e",
            "917197064e794e80a57e09b905a9fb42",
            "590f839ccd274dd8ae0c2bafa3f81963",
            "18163463962447ac9469c00744e46a0a",
            "e99d06065fd748e799051b465b55b6a7",
            "9d8e6adeaa2d4ad0b688c54c5b3b80cc",
            "3510f0b683ef43d78357520bee775576",
            "8184a7d58b1442238b8a72e9c4f92085",
            "ff89b4d3381247cba2637c3224b4bac7",
            "2102faa0f4c54c8cbcee7f098013c357",
            "fcf0d28d90a643b88f9e8b3d96d39393",
            "1cf56661d06e471e8f05bb6c07f0df8e",
            "48c0801823cd45ffb67ed4bd0c039eee",
            "16e253c7a4fa44b093f97aacaaf68ce4",
            "b75a0303b8474473adaae83ff583037e",
            "a3d42ccd30714e60a847191c59b0d787",
            "7be7853d4c6d4b89a34b54346edc1840",
            "9de964568bd542f9ad0ac7e56168b5d7",
            "0b4b6009cd584f5b80bff44c684c013c",
            "24a78a35f5a24825a22f255448a51a94",
            "bffdedcf978944c99ce65e5b92973abd",
            "46fc45b5781641229e8550a79972a5b3",
            "a9f0d9c80b504ec8be3e2a5e932339b7",
            "a3d98d5b38a047e9aa9672e68c83b60e",
            "4fbc0e35556d4082a13de7b896ac0136",
            "1459b699864d49b2ac59a23e94a7a05a",
            "4a20d40f8e854cfe843a900053326fa7",
            "52a9bf7c486d43bfa51f554f1d07f3fa",
            "15351d5e520d4149ae33592edc232bd2",
            "d172064584ff482988aece3877ecbc6d",
            "37857afcefd24bbeae6b02e5025ed710",
            "51187c5e68a2498e91b940601f976bd5",
            "dfe6d101b61b4439973d9b9a786c14cd",
            "683bef9dce9a413c917349e625286df0",
            "68ce8e17eaff45a5943ee0cbf5fd3e00",
            "d7912e1bb373441b80ee594997b01b98",
            "77b25987406e46fe85d10d9527bc5996",
            "c2107d0c94e849a68a85922b16d02b55",
            "430050d507a144ec9cedd332fb799bb5",
            "208bf3f957984e8db4b5e85cb8b67cae",
            "4abe79b6ddee41a69fd8cd725dab3541",
            "de4737c09d554eed8d47ef89e72a4431",
            "00fbc03859134b5880a6af56282a2ea2",
            "19164dcf963e4dd89d90e95098892c9a",
            "c88215d52b9848c5b22ca35adba5ebdb",
            "9c715f3a82c84b32a8649a9cfff8d197",
            "38984dbbb862427583e061eeda3e5d07",
            "103da345f9fe473cae667b311dd1eaed",
            "c1af124189034a40aac9a8f665e1df3e",
            "f8599dfafbaf412fb21c1d44f617d16c",
            "248a35c04a0044e18e3c2b692ffcd976",
            "2aa6c190192e496ba3aa8684e6673352"
          ]
        },
        "id": "qVR6zx2v8-Jy",
        "outputId": "40ea8cea-8e28-4575-c112-dd851e1f65fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Applying formatting function to train dataset:   0%|          | 0/482 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aad446ee48f04ff7891a5f71d3f54710"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Converting train dataset to ChatML:   0%|          | 0/482 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ff89b4d3381247cba2637c3224b4bac7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/482 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24a78a35f5a24825a22f255448a51a94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/482 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37857afcefd24bbeae6b02e5025ed710"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/482 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "de4737c09d554eed8d47ef89e72a4431"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and save\n",
        "trainer.train()\n",
        "trainer.model.save_pretrained(\"e-doctor-qlora-adapter\")\n",
        "tokenizer.save_pretrained(\"e-doctor-qlora-adapter\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "CyHHMxls8_sy",
        "outputId": "b11e4f86-e86e-47a7-f59b-2ef08dd0a589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 11:14, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.362300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.439200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.323500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.361000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.383300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.359000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('e-doctor-qlora-adapter/tokenizer_config.json',\n",
              " 'e-doctor-qlora-adapter/special_tokens_map.json',\n",
              " 'e-doctor-qlora-adapter/chat_template.jinja',\n",
              " 'e-doctor-qlora-adapter/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r e-doctor-qlora-adapter e-doctor-qlora-adapter\n"
      ],
      "metadata": {
        "id": "u0qpJz-k9BDg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03e385e-dedc-415d-f785-37edae9d68a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: e-doctor-qlora-adapter/ (stored 0%)\n",
            "  adding: e-doctor-qlora-adapter/tokenizer.json (deflated 85%)\n",
            "  adding: e-doctor-qlora-adapter/chat_template.jinja (deflated 75%)\n",
            "  adding: e-doctor-qlora-adapter/adapter_model.safetensors (deflated 7%)\n",
            "  adding: e-doctor-qlora-adapter/adapter_config.json (deflated 56%)\n",
            "  adding: e-doctor-qlora-adapter/tokenizer_config.json (deflated 96%)\n",
            "  adding: e-doctor-qlora-adapter/special_tokens_map.json (deflated 65%)\n",
            "  adding: e-doctor-qlora-adapter/README.md (deflated 66%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"e-doctor-qlora-adapter.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jBV-uYq8IXs1",
        "outputId": "f87926bd-16cc-4aa7-a937-31408746cf2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_77bc07a6-e862-433c-9ca1-a6c3b51cc382\", \"e-doctor-qlora-adapter.zip\", 158019755)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PROMPT_TEMPLATE = \"\"\"You are a conversational AI assistant acting as a professionally worded \"e-doctor.\" Your role is to provide preliminary, text-based guidance in response to patient symptom-related questions.\n",
        "\n",
        "### Patient Query:\n",
        "{question}\n",
        "\n",
        "### AI Doctor Response:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "xylFnNESRtJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_e_doctor_response(patient_query, model, tokenizer,\n",
        "                         max_new_tokens=100, temperature=0.7):\n",
        "    # Format the prompt with the patient query\n",
        "    prompt = PROMPT_TEMPLATE.format(question=patient_query)\n",
        "    # Tokenize and move to the model's device\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    # Generate the response\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=temperature\n",
        "        )\n",
        "    # Decode and return only the generated response (after the prompt)\n",
        "    full_output = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    response = full_output.split(\"### AI Doctor Response:\")[-1].strip()\n",
        "    return response\n"
      ],
      "metadata": {
        "id": "AYgSwjJHUMuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patient_query = \"i have tuberculosis what should i do?\"\n",
        "response = get_e_doctor_response(patient_query, model, tokenizer)\n",
        "print(\"AI Doctor:\", response)\n"
      ],
      "metadata": {
        "id": "DALsnbuFUPg2",
        "outputId": "eab9a0f8-bed4-47de-b08f-fd9668f52feb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Doctor: in brief: treatment tuberculosis requires specific treatment. you can only be diagnosed by a doctor on video or phone (telephonically). a chest x-ray is needed. the treatment depends on the form of tuberculosis. would you like to video or text chat with me?\n",
            "\n",
            "*Disclaimer: This response is generated for informational purposes only and should not be considered a substitute for professional medical advice, diagnosis, or treatment. Please consult a licensed healthcare provider for personalized care.*\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "patient_query = \"dry scratchy throat, very mild cough since 23 march but not improving. traveled to cape town via lanseria and ct airports 15-20 march. wife has sore throat, nasal congestion, lethargy & headaches since 21 march. should we see a doctor to be assessed?\"\n",
        "\n",
        "response = get_e_doctor_response(patient_query, model, tokenizer)\n",
        "print(\"AI Doctor:\", response)\n"
      ],
      "metadata": {
        "id": "kNY16EbXUbdl",
        "outputId": "f86b4f11-8a10-4247-b417-39ce9dbc7296",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Doctor: yes. if you have access to a general physician, discuss your symptoms and see if a chest x-ray or some other diagnostic testing would be helpful. if you don't have access to a general physician, call your local emergency services and discuss the situation. covid-19 symptoms can be mild or severe. it's essential to contact a doctor or other healthcare professional to be screened for covid-19 if you have symptoms.\n",
            "\n",
            "*Disclaimer: This response is generated for informational purposes only and should not be considered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z_KxJv_RUmsb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}