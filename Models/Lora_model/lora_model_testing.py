# -*- coding: utf-8 -*-
"""LORA_Model_testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JIVi95w16WFIOR7PS8bqC2--v4lAVgb7
"""

!pip install unsloth # install unsloth
!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!

#Import necessary libraries
from unsloth import FastLanguageModel
import torch
from trl import SFTTrainer
from unsloth import is_bfloat16_supported
from huggingface_hub import login
from transformers import TrainingArguments
from datasets import load_dataset
import wandb

from google.colab import drive
drive.mount('/content/drive')

!unzip "/content/drive/My Drive/e_doctor_lora.zip" -d /content/

from unsloth import FastLanguageModel
from transformers import AutoTokenizer

model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"  # base model
max_seq_length = 2048
load_in_4bit = True


model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    load_in_4bit = load_in_4bit,
    dtype = None,
)

# Apply LoRA weights
model.load_adapter("/content/e_doctor_lora")  #saved fine-tuned adapter

# Define your updated prompt style
train_prompt_style = """Below is a task designed to simulate a responsible, medically informed AI assistant ("e-doctor") that offers preliminary guidance for common patient inquiries. The AI assistant is trained to follow recognized clinical guidelines, use professional language, and always clarify that it does not replace a licensed healthcare provider.

### Instruction:
You are a virtual medical assistant providing preliminary, text-based health advice and recommend some medicine. Analyze the question carefully, think through the relevant clinical reasoning, and craft a formal response that is informative, safe, and contextually appropriate and talk like that patient feel you are real doctor and recommend some medicine also.

### Patient Query:
{}

### AI Assistant Response:
{}

*Disclaimer: This response is generated for informational purposes only and is not a substitute for professional medical advice, diagnosis, or treatment. Please consult a licensed healthcare provider for personalized care.*
"""

import torch

FastLanguageModel.for_inference(model)

question = "i have Tuberculosis what should i do?"
prompt = train_prompt_style.format(question,"")

# Tokenize
inputs = tokenizer([prompt], return_tensors="pt").to("cuda")

# Generate
outputs = model.generate(
    input_ids=inputs["input_ids"],
    attention_mask=inputs["attention_mask"],
    max_new_tokens=150,
    do_sample=True,
    temperature=0.1,
    top_p=0.7
)

# Decode
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
final_response = response.split("### AI Assistant Response:")[-1].strip()
print(final_response)

import torch


FastLanguageModel.for_inference(model)

system_prompt = (
    "You are 'e-doctor', an AI medical assistant. "
    "You provide preliminary, guideline-based, and talk professionally  to patients and provide them proper treatment and suggest medicine' medical questions. "
    "first start with telling about the diseases and examine their symtoms and then suggest them some medicine acc to symtoms do all that professionally it feels like a real doctor to a patient"
    "You use formal language, show empathy, and always end with: "
    "'This advice is for informational purposes only and does not replace professional medical care.' "
    "Do not repeat previous answers. Address the patient's current question specifically.\n"
)


conversation_history = []

print("AI Medical Assistant Chat (type 'exit' to stop):\n")
while True:
    user_input = input("Patient: ")
    if user_input.lower() == "exit":
        break


    conversation_history.append(f"Patient: {user_input}")

    # Build the prompt with only the last 2 turns (for context and to avoid repetition)
    max_turns = 2
    history_str = "\n".join(conversation_history[-max_turns*2:])

    # Final prompt
    prompt = f"{system_prompt}{history_str}\nDoctor:"

    # Tokenize input
    inputs = tokenizer([prompt], return_tensors="pt", truncation=True).to("cuda")

    # Generate response
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        attention_mask=inputs["attention_mask"],
        max_new_tokens=120,
        do_sample=True,
        temperature=0.7,
        top_p=0.7,
        pad_token_id=tokenizer.eos_token_id
    )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    final_response = response.split("Doctor:")[-1].strip().split("Patient:")[0].strip()

    print(f"Doctor: {final_response}")

    conversation_history.append(f"Doctor: {final_response}")

