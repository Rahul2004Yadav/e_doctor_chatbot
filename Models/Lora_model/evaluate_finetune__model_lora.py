# -*- coding: utf-8 -*-
"""evaluate_finetune _model_lora.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qTFOmBw4Ih7bzQ3HkekH4dp0VFKuUk6q
"""

!unzip e_doctor_lora.zip -d e_doctor_lora/

!pip install torch transformers bitsandbytes accelerate

!pip install unsloth # install unsloth
!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!

# Step3: Import necessary libraries
from unsloth import FastLanguageModel
import torch
from trl import SFTTrainer
from unsloth import is_bfloat16_supported
from huggingface_hub import login
from transformers import TrainingArguments
from datasets import load_dataset
import wandb

from unsloth import FastLanguageModel
from transformers import AutoTokenizer

model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"  # Your base model
max_seq_length = 2048
load_in_4bit = True

# Load base model + tokenizer
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_seq_length,
    load_in_4bit = load_in_4bit,
    dtype = None,  # or torch.float16 / torch.bfloat16
)

# Apply LoRA weights from your saved adapter
model.load_adapter("/content/e_doctor_lora/e_doctor_lora")  # âœ… Loads your saved fine-tuned adapter

import torch
import math

def calculate_perplexity(model, tokenizer, texts, device="cuda"):
    model.eval()
    # model.to(device)
    perplexities = []
    with torch.no_grad():
        for text in texts:
            encoding = tokenizer(text, return_tensors="pt")
            input_ids = encoding.input_ids.to(device)
            # Some Unsloth models may require attention_mask
            attention_mask = encoding.get("attention_mask", None)
            if attention_mask is not None:
                attention_mask = attention_mask.to(device)
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)
            else:
                outputs = model(input_ids=input_ids, labels=input_ids)
            # Handle output as tuple or object
            loss = outputs.loss if hasattr(outputs, "loss") else outputs[0]
            ppl = math.exp(loss.item())
            perplexities.append(ppl)
    return perplexities

# Example usage:
test_texts = [
    "the patient has symptoms of acute myocardial infarction and needs urgent care.",

]

device = "cuda" if torch.cuda.is_available() else "cpu"
ppls = calculate_perplexity(model, tokenizer, test_texts, device)
print(ppls)

import json
import torch
from unsloth import FastLanguageModel
from transformers import AutoTokenizer

# 1. Load your test data
with open('/content/test3.json', 'r') as f:
    test_data = json.load(f)

# # 2. Load base model + tokenizer
# model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
# max_seq_length = 2048
# load_in_4bit = True

# model, tokenizer = FastLanguageModel.from_pretrained(
#     model_name = model_name,
#     max_seq_length = max_seq_length,
#     load_in_4bit = load_in_4bit,
#     dtype = None,  # or torch.float16 / torch.bfloat16
# )

# # 3. Load your LoRA adapter weights
# model.load_adapter("/content/e_doctor_lora/e_doctor_lora")  # Path to your LoRA adapter

# 4. Device setup
device = 'cuda' if torch.cuda.is_available() else 'cpu'
# Do NOT call model.to(device) for quantized models!

# 5. Generation function
def generate_answer(question, tokenizer, model, device='cuda'):
    prompt = f"Patient: {question}\nDoctor:"
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model.generate(
            input_ids=inputs.input_ids,
            attention_mask=inputs.attention_mask,
            max_new_tokens=128,
            do_sample=False
        )
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # Extract only the doctor's answer
    if "Doctor:" in decoded:
        answer = decoded.split("Doctor:")[-1].strip()
    else:
        answer = decoded.strip()
    return answer

# 6. Generate answers for your test set
references = []
candidates = []

for item in test_data:
    question = item['input']
    reference = item['output']
    candidate = generate_answer(question, tokenizer, model, device)
    references.append(reference)
    candidates.append(candidate)

# Now references and candidates contain your ground-truth and model-generated answers.
# You can use them for ROUGE or other evaluation.

for ref, cand in zip(references[:5], candidates[:5]):
    print("Reference:", ref)
    print("Candidate:", cand)
    print("-" * 40)

!pip install rouge-score

from rouge_score import rouge_scorer
import numpy as np

scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
rouge1_scores, rouge2_scores, rougeL_scores = [], [], []

for ref, cand in zip(references, candidates):
    scores = scorer.score(ref, cand)
    rouge1_scores.append(scores['rouge1'].fmeasure)
    rouge2_scores.append(scores['rouge2'].fmeasure)
    rougeL_scores.append(scores['rougeL'].fmeasure)

print(f"Average ROUGE-1: {np.mean(rouge1_scores):.4f}")
print(f"Average ROUGE-2: {np.mean(rouge2_scores):.4f}")
print(f"Average ROUGE-L: {np.mean(rougeL_scores):.4f}")

import torch
import time

def measure_latency(model, tokenizer, prompt, device='cuda', num_runs=10):

    inputs = tokenizer(prompt, return_tensors='pt').to(device)

    for _ in range(3):
        _ = model.generate(**inputs, max_new_tokens=50)

    torch.cuda.synchronize() if device == "cuda" else None
    start_time = time.time()
    for _ in range(num_runs):
        _ = model.generate(**inputs, max_new_tokens=50)
    torch.cuda.synchronize() if device == "cuda" else None
    end_time = time.time()
    avg_latency = (end_time - start_time) / num_runs
    return avg_latency * 1000


prompt = "Patient: I have a headache and fever. What should I do?\nDoctor:"
device = "cuda" if torch.cuda.is_available() else "cpu"
avg_latency = measure_latency(model, tokenizer, prompt, device)
print(f'Average inference latency (ms): {avg_latency:.2f}')

import os

def get_model_size(model_path):
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(model_path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            total_size += os.path.getsize(fp)
    return total_size / (1024 * 1024)

model_path = '/content/e_doctor_lora/e_doctor_lora'
model_size_mb = get_model_size(model_path)
print(f"Model size (MB): {model_size_mb:.2f}")

import pandas as pd

metrics = ['ROUGE-1', 'ROUGE-2', 'PPL', 'Latency (ms)', 'Model Size (MB)']
values = [0.1811, 0.0212, 36.11551720221854, 5157.7, 176.53]

description = [
    'Unigram overlap (measures recall of important words)',
    'Bigram overlap (measures recall of important word pairs)',
    'reflects model fluency and prediction quality',
    'Inference time per token',
    'Disk size of the fine-tuned model (smaller is more deployable)'
]

comparative_results_df = pd.DataFrame({
    'Metric': metrics,
    'Value': values,
    'Description': description
})


print(comparative_results_df.to_markdown(index=False))

