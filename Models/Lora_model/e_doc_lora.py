# -*- coding: utf-8 -*-
"""e_doc_LORA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iH_JuXp__SrpStESUkfS7fqdsQo7n1c0
"""

!pip install unsloth # install unsloth
!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git

# Step3: Import necessary libraries
from unsloth import FastLanguageModel
import torch
from trl import SFTTrainer
from unsloth import is_bfloat16_supported
from huggingface_hub import login
from transformers import TrainingArguments
from datasets import load_dataset
import wandb

from google.colab import userdata
hf_token = userdata.get('HF_TOKEN')
login(hf_token)

import torch
print("CUDA available:", torch.cuda.is_available())
print("GPU device:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")

model_name = "deepseek-ai/DeepSeek-R1-Distill-Llama-8B"
max_sequence_length = 1024
dtype = None
load_in_4bit = True

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = model_name,
    max_seq_length = max_sequence_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    token = hf_token
)

prompt_style= """
You are a conversational AI assistant acting as a professionally worded “e-doctor.” Your role is to provide preliminary, text-based guidance in response to patient symptom-related questions. You must **not** substitute for a licensed medical professional.

Before responding, analyze the user’s input carefully. Follow a structured reasoning process and ensure the output:

- Adheres to established clinical guidelines or best practices.
- Is phrased in clear, formal, and empathetic medical language.
- Includes appropriate disclaimers indicating the informational nature of the advice.

### Objective:
Healthcare providers are adopting AI-assisted tools to support triage and early guidance. This system is designed to simulate a responsible AI health assistant trained on a medically-relevant dataset using decoder-only transformer models. It compares three fine-tuning approaches for training: Prompt Tuning, LoRA/QLoRA with PEFT, and Full Fine-Tuning.

### Task:
Respond to the medical inquiry below by generating a high-quality, medically sound answer that balances informativeness and caution.

### Input (Patient Query):
{}

### Output (AI Assistant Response):
{}

*Disclaimer: This response is generated for informational purposes only and should not be considered a substitute for professional medical advice, diagnosis, or treatment. Please consult a licensed healthcare provider for personalized care.*
"""

question = """A 61-year-old woman with a long history of involuntary urine loss during activities like coughing or
              sneezing but no leakage at night undergoes a gynecological exam and Q-tip test. Based on these findings,
              what would cystometry most likely reveal about her residual volume and detrusor contractions?"""

FastLanguageModel.for_inference(model)

# Tokenize the input
inputs = tokenizer([prompt_style.format(question, "")], return_tensors="pt").to("cuda")

# Generate a response
outputs = model.generate (
    input_ids = inputs.input_ids,
    attention_mask = inputs.attention_mask,
    max_new_tokens = 1200,
    use_cache = True
)

# Decode the response tokens back to text
response = tokenizer.batch_decode(outputs)


print(response)

print(response[0].split("### Output (AI Assistant Response):")[1])

import json
from datasets import Dataset


with open("/content/combined_output2_final.json", "r") as f:
    raw_data = json.load(f)

# Build input-output pairs
formatted_data = []
for entry in raw_data:
    utterances = entry.get("utterances", [])
    if len(utterances) >= 2:
        patient_utterance = utterances[0].replace("patient:", "").strip()
        doctor_response = utterances[1].replace("doctor:", "").strip()

        prompt_style = f"""Given the following patient query, provide a helpful and informative response from the perspective of a doctor.\n\nPatient query: {patient_utterance}\n\nYour response:"""

        formatted_data.append({
            "input": prompt_style,
            "output": doctor_response
        })

# Create Hugging Face dataset
hf_dataset = Dataset.from_list(formatted_data)

hf_dataset[10]

EOS_TOKEN = tokenizer.eos_token  # Define EOS_TOKEN which tells the model when to stop generating text during training
EOS_TOKEN

train_prompt_style = """Below is a task designed to simulate a responsible, medically informed AI assistant ("e-doctor") that offers preliminary guidance for common patient inquiries. The AI assistant is trained to follow recognized clinical guidelines, use professional language, and always clarify that it does not replace a licensed healthcare provider.

### Instruction:
You are a virtual medical assistant providing preliminary, text-based health advice. Analyze the question carefully, think through the relevant clinical reasoning, and craft a formal response that is informative, safe, and contextually appropriate.

### Patient Query:
{}

### AI Assistant Response:
{}

*Disclaimer: This response is generated for informational purposes only and is not a substitute for professional medical advice, diagnosis, or treatment. Please consult a licensed healthcare provider for personalized care.*
"""

hf_dataset.features

train_prompt_style = """Below is a task designed to simulate a responsible, medically informed AI assistant ("e-doctor") that offers preliminary guidance for common patient inquiries. The AI assistant is trained to follow recognized clinical guidelines, use professional language, and always clarify that it does not replace a licensed healthcare provider.

### Instruction:
You are a virtual medical assistant providing preliminary, text-based health advice. Analyze the question carefully, think through the relevant clinical reasoning, and craft a formal response that is informative, safe, and contextually appropriate.

### Patient Query:
{}

### AI Assistant Response:
{}

*Disclaimer: This response is generated for informational purposes only and is not a substitute for professional medical advice, diagnosis, or treatment. Please consult a licensed healthcare provider for personalized care.*
"""

# EOS_TOKEN = "</s>"


def preprocess_input_data(example):
    inputs = example["input"]
    outputs = example["output"]

    # Extract patient query (assuming it's after "Patient query:")
    if "Patient query:" in inputs:
        patient_query = inputs.split("Patient query:")[-1].strip()
    else:
        patient_query = inputs.strip()

    # Format into training text
    texts = train_prompt_style.format(patient_query, outputs) + EOS_TOKEN

    return {
        "text": texts
    }


finetune_dataset = hf_dataset.map(preprocess_input_data)

finetune_dataset["text"][0]

from unsloth import FastLanguageModel

# Apply LoRA to your decoder-only model
model_lora = FastLanguageModel.get_peft_model(
    model = model,
    r = 16,  # LoRA rank
    target_modules = [
        "q_proj", "k_proj", "v_proj", "o_proj",
        "gate_proj", "up_proj", "down_proj"
    ],
    lora_alpha = 16,
    lora_dropout = 0.05,
    bias = "none",
    use_gradient_checkpointing = "unsloth",
    random_state = 3047,
    use_rslora = False,
    loftq_config = None
)

if hasattr(model, '_unwrapped_old_generate'):
    del model._unwrapped_old_generate

trainer = SFTTrainer(
    model = model_lora,
    tokenizer = tokenizer,
    train_dataset = finetune_dataset,
    dataset_text_field = "texts",
    max_seq_length = 1024,
    dataset_num_proc = 1,

    # Define training args
    args = TrainingArguments(
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    num_train_epochs=1,
    warmup_steps=5,
    max_steps=60,
    learning_rate=2e-4,
    fp16=False,
    bf16=False,
    logging_steps=10,
    optim="adamw_8bit",
    weight_decay=0.01,
    lr_scheduler_type="linear",
    seed=3407,
    output_dir="outputs",


    ),
)

#  Start fine-tuning
trainer.train()
# trainer.model.save_pretrained("e_doctor_lora")
# tokenizer.save_pretrained("e_doctor_lora")

wandb.finish()

# Save only the LoRA adapter weights (small & fast)
trainer.model.save_pretrained("e_doctor_lora")
tokenizer.save_pretrained("e_doctor_lora")

!zip -r e_doctor_lora.zip e_doctor_lora

from google.colab import files
files.download("e_doctor_lora.zip")

